{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEANED CONTENT FILE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_claim_info(claims):\n",
    "    claims = re.sub(r'c-en-01-(\\d{4})', 'c-en-\\\\1', claims)\n",
    "    claims = re.sub(r'(id=\"c-en-(\\d{4})\") num=\"\"', r'\\1 num=\"\\2\"', claims)\n",
    "    claim_regex = r'(?:<clai)?m id=\"c-en-(\\d+)\" num=\"(\\d+)\">(.*?)</claim>'\n",
    "    claim_text_regex = r'<claim-text>(.*?)</claim-text>'\n",
    "    # Extracting all claims from the json_mapper\n",
    "    all_claims = re.findall(claim_regex, claims, re.DOTALL)\n",
    "    claim_info = {}\n",
    "    claim_list = []\n",
    "    for claim_id, claim_num, claim_content in all_claims:\n",
    "        claim_list.append(int(claim_num))\n",
    "        claim_texts = re.findall(claim_text_regex, claim_content, re.DOTALL)\n",
    "        claim_text = ' '.join(claim_texts).strip()\n",
    "        claim_text = claim_text.replace(\"<claim-text>\", \" \")\n",
    "        claim_text = claim_text.replace(\"</claim-text>\", \" \")\n",
    "        claim_text = re.sub(r'<[^>]+>', '', claim_text).strip()\n",
    "        claim_info[f\"c-en-{int(claim_num):04d}\"] = claim_text\n",
    "    return {f\"c-en-{num:04d}\": claim_info.get(f\"c-en-{num:04d}\", \"Claim text not found\") for num in claim_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(description):\n",
    "    extracted_paragraphs = {}\n",
    "    paragraph_pattern = re.compile(r'<p id=\"p(\\d+)\"[^>]*>(.*?)</p>', re.DOTALL)\n",
    "    matches = paragraph_pattern.findall(description)\n",
    "    for pid, pcontent in matches:\n",
    "        # Clean up <figref> tags and other HTML tags from paragraphs\n",
    "        pcontent_clean = re.sub(r'<figref idref=\"[^\"]+\">(.*?)</figref>', r'\\1', pcontent)\n",
    "        pcontent_clean = re.sub(r'<[^>]+>', '', pcontent_clean).strip()\n",
    "        extracted_paragraphs[f\"p{int(pid):04d}\"] = pcontent_clean\n",
    "    return extracted_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_cleaner(title, description, abstract=None, claims=None):\n",
    "    dict_content = {}\n",
    "    title = title.replace(\"\\n\", \"\")\n",
    "    dict_content.update({\"title\": title})\n",
    "    if abstract:\n",
    "        abstract_clean = re.sub(r'<[^>]+>', '', abstract).strip()[2:]\n",
    "        dict_content.update({\"pa01\": abstract_clean})\n",
    "    if claims:\n",
    "        all_claims = extract_claim_info(claims)\n",
    "        dict_content.update(all_claims)\n",
    "    dict_content.update(extract_paragraphs(description))\n",
    "    return dict_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patents processed: 12195\n"
     ]
    }
   ],
   "source": [
    "def process_data(json_citing, q7_info, type_doc=None, cited=True):\n",
    "    data = []\n",
    "    counter_total = 0\n",
    "    seen_set = set()\n",
    "    matching_entry = None\n",
    "    for patent in json_citing:\n",
    "        application = patent[\"application_number\"] + patent[\"application_category\"]\n",
    "        if type_doc:\n",
    "            matching_entry = next(\n",
    "                (element for element in q7_info if (patent[\"application_number\"] + patent[\"application_category\"] == element[type_doc]) and (element['Category_Cited'] == 'X' or element['Category_Cited'] == 'A') and element[\"Content_Cited\"]), None\n",
    "            )\n",
    "        if matching_entry or not cited:\n",
    "            if (cited and application not in seen_set) or not cited:\n",
    "                seen_set.add(application)\n",
    "                quadruple_dict = {\n",
    "                    \"Application_Number\": patent[\"application_number\"],\n",
    "                    \"Application_Date\": patent[\"application_date\"],\n",
    "                    \"Application_Category\": patent[\"application_category\"],\n",
    "                    \"Content\": content_cleaner(\n",
    "                        patent[\"title\"],\n",
    "                        patent[\"description\"],\n",
    "                        abstract=patent.get(\"abstract\"),\n",
    "                        claims=patent.get(\"claims\")\n",
    "                    )\n",
    "                }\n",
    "                data.append(quadruple_dict)\n",
    "                counter_total += 1 \n",
    "                if not cited and counter_total == 8000:\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    print(f\"Total number of patents processed: {len(data)}\")\n",
    "    return data\n",
    "\n",
    "def open_json_file(path_to_json):\n",
    "    # Opening the JSON file\n",
    "    with open(path_to_json, 'r') as file:\n",
    "        # Loading JSON\n",
    "        json_dict = json.load(file)\n",
    "    return json_dict\n",
    "\n",
    "def data_write(data, directory, path):\n",
    "    # Creation of Directory if doesn't exist\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # File Path to store data information\n",
    "    file_path = os.path.join(directory, path)\n",
    "    # Writing Q7 Information on JSON\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Converting from list of dictionaries to JSON + indent=4 for readability\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def create_json(path, data1, data2=None): # data1 = uncited, data2 = cited for 2-nd json\n",
    "    if data2:\n",
    "        merged_data = data1 + data2 # Merging both data\n",
    "        directory = '/bigstorage/DATASETS_JSON/Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k'\n",
    "        data_write(merged_data, directory, 'CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json')\n",
    "    else:\n",
    "        directory = '/bigstorage/DATASETS_JSON/Content_JSONs/Citing_2020_Cleaned_Content_12k'\n",
    "        data_write(data1, directory, 'CLEANED_CONTENT_DATASET_citing_patents_2020.json')\n",
    "\n",
    "def main():\n",
    "    path_to_directory_citing = \"/bigstorage/DATASETS_JSON/Content_JSONs/Citing_2020_Cleaned_Content_12k\"\n",
    "    path_to_directory_cited = \"/bigstorage/DATASETS_JSON/Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k\"\n",
    "\n",
    "    path_to_json_citing ='/bigstorage/DATASETS_JSON/Base_JSONS/CONTENT_DATASET_citing_patents_2020.json'\n",
    "    path_to_json_uncited ='/bigstorage/DATASETS_JSON/Base_JSONS/CONTENT_DATASET_uncited_patents.json'\n",
    "    path_to_json_cited ='/bigstorage/DATASETS_JSON/Base_JSONS/CONTENT_DATASET_cited_patents_by_2020.json'\n",
    "    path_to_q7 = '/bigstorage/DATASETS_JSON/Q7Info/Q7.json'\n",
    "\n",
    "    json_citing = open_json_file(path_to_json_citing)\n",
    "    json_cited = open_json_file(path_to_json_cited)\n",
    "    json_uncited = open_json_file(path_to_json_uncited)\n",
    "    json_q7 = open_json_file(path_to_q7)\n",
    "\n",
    "    data_citing = process_data(json_citing, json_q7, type_doc=\"ID_Citing\")\n",
    "    data_cited = process_data(json_cited, json_q7, type_doc=\"ID_Cited\")\n",
    "    data_uncited = process_data(json_uncited, json_q7, cited=False)\n",
    "    create_json(path_to_directory_citing, data_citing)\n",
    "    create_json(path_to_directory_cited, data_cited, data_uncited)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CITATIONS TRAIN-TEST FILES CREATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_citations(json_citing, cat_type):\n",
    "    data = []\n",
    "    counter_total = 0\n",
    "    counter_type = 0\n",
    "    for patent in json_citing:\n",
    "        if patent[\"Category_Cited\"] == cat_type and patent.get(\"Claims_Text\") and patent.get(\"Content_Cited\"):\n",
    "            quadruple_tuple = (\n",
    "                patent[\"ID_Citing\"],\n",
    "                list(patent.get(\"Claims_Text\").keys()),\n",
    "                patent[\"ID_Cited\"],\n",
    "                list(patent.get(\"Content_Cited\").keys()),\n",
    "                patent[\"Category_Cited\"]\n",
    "            )\n",
    "            data.append(quadruple_tuple)\n",
    "            counter_total += 1\n",
    "            counter_type += 1\n",
    "    print(f\"Total number of patents with type {cat_type}: {counter_type}\")\n",
    "    print(f\"Total number of patents processed: {counter_total}\")\n",
    "    return data, counter_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patents with type X: 3671\n",
      "Total number of patents processed: 3671\n",
      "Total number of patents with type A: 6189\n",
      "Total number of patents processed: 6189\n",
      "3671\n",
      "6189\n"
     ]
    }
   ],
   "source": [
    "def open_json_file(path_to_json):\n",
    "    # Opening the JSON file\n",
    "    with open(path_to_json, 'r') as file:\n",
    "        # Loading JSON\n",
    "        json_dict = json.load(file)\n",
    "    return json_dict\n",
    "\n",
    "path_to_new_json = \"/bigstorage/DATASETS_JSON/Citing_2020_Cleaned_Content_12k/\"\n",
    "path_to_q7 ='/bigstorage/DATASETS_JSON/Q7Info/Q7.json'\n",
    "json_citing = open_json_file(path_to_q7)\n",
    "data_X = process_data_for_citations(json_citing, \"X\")\n",
    "data_A = process_data_for_citations(json_citing, \"A\")\n",
    "\n",
    "print(data_X[1])\n",
    "print(data_A[1])\n",
    "#create_json(data, path_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6831\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Total items': 9860,\n",
       " 'List 1 total items': 8860,\n",
       " 'List 2 total items': 1000,\n",
       " 'List 1 type A items': 5578,\n",
       " 'List 1 type X items': 3282,\n",
       " 'List 2 type A items': 611,\n",
       " 'List 2 type X items': 389,\n",
       " 'List 1 type A ratio': 0.6295711060948082,\n",
       " 'List 1 type X ratio': 0.37042889390519185}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_data = data_A[0] + data_X[0]\n",
    "random.shuffle(mock_data)\n",
    "\n",
    "# Group documents by 'id_citing'\n",
    "docs_by_id_citing = defaultdict(list)\n",
    "for doc in mock_data:\n",
    "    docs_by_id_citing[doc[0]].append(doc)\n",
    "\n",
    "# Sort groups by their size to try filling the test dataset with smaller groups first\n",
    "sorted_groups = sorted(docs_by_id_citing.items(), key=lambda x: len(x[1]))\n",
    "\n",
    "list_1 = []  # Train\n",
    "list_2 = []  # Test\n",
    "\n",
    "type_A_count_list_1 = 0\n",
    "type_X_count_list_1 = 0\n",
    "\n",
    "type_A_count_list_2 = 0\n",
    "type_X_count_list_2 = 0\n",
    "\n",
    "# First try to fill the test set to have exactly 1000 documents\n",
    "for id_citing, docs in sorted_groups:\n",
    "    if len(list_2) + len(docs) <= 1000:  # Ensure test set does not exceed 1000 documents\n",
    "        list_2.extend(docs)\n",
    "        for doc in docs:\n",
    "            if doc[4] == \"A\":\n",
    "                type_A_count_list_2 += 1\n",
    "            else:\n",
    "                type_X_count_list_2 += 1\n",
    "    else:\n",
    "        # Once the test set is filled, start filling the train set\n",
    "        list_1.extend(docs)\n",
    "        for doc in docs:\n",
    "            if doc[4] == \"A\":\n",
    "                type_A_count_list_1 += 1\n",
    "            else:\n",
    "                type_X_count_list_1 += 1\n",
    "\n",
    "\n",
    "distribution_info = {\n",
    "    \"Total items\": len(mock_data),\n",
    "    \"List 1 total items\": len(list_1),\n",
    "    \"List 2 total items\": len(list_2),\n",
    "    \"List 1 type A items\": type_A_count_list_1,\n",
    "    \"List 1 type X items\": type_X_count_list_1,\n",
    "    \"List 2 type A items\": type_A_count_list_2,\n",
    "    \"List 2 type X items\": type_X_count_list_2,\n",
    "    \"List 1 type A ratio\": type_A_count_list_1 / len(list_1),\n",
    "    \"List 1 type X ratio\": type_X_count_list_1 / len(list_1)\n",
    "}\n",
    "distribution_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_citations(data, path):\n",
    "    directory = \"/bigstorage/DATASETS_JSON/Citation_JSONs/\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    file_path = os.path.join(directory, 'Citation_Train.json')\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Converting from list of dictionaries to JSON + indent=4 for readability\n",
    "        json.dump(data, file, indent=4) \n",
    "\n",
    "path_to_citation_json = \"/bigstorage/DATASETS_JSON/Citation_JSONs\"\n",
    "create_json_citations(list_1, path_to_citation_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
